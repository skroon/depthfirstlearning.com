<p>Thank you to Kumar Krishna Agrawal, Yasaman Bahri, Peter Chen, Nic Ford, Roy Frostig, Xinyang Geng, Rein Houthooft, Ben Poole, Colin Raffel and Supasorn Suwajanakorn for contributing to this guide.</p>

<div class="deps-graph">
  <iframe class="deps" src="/assets/infogan-deps.svg" width="200"></iframe>
  <div>Concepts used in InfoGAN. Click to navigate.</div>
</div>

<h1 id="why">Why</h1>

<p>InfoGAN is an extension of GANs that learns to represent unlabeled data as codes,
aka representation learning. Compare this to vanilla GANs that can only generate 
samples or to VAEs that learn to both generate code and samples. Representation 
learning is an important direction for unsupervised learning and GANs are a 
flexible and powerful interpretation. This makes InfoGAN an interesting stepping 
stone towards research in representation learning.</p>

<p><a href="https://colab.research.google.com/drive/1JkCI_n2U2i6DFU8NKk3P6EkPo3ZTKAaq#forceEdit=true&amp;offline=true&amp;sandboxMode=true" class="colab-root">Reproduce in a
    <span>Notebook</span></a></p>

<p><br /></p>

<h1 id="1-information-theory">1 Information Theory</h1>
<p><strong>Motivation</strong>: Information theory formalizes the concept of the “amount of randomness” or 
“amount of information”. These concepts can be extended to relative quantities 
among random variables. This section leads to Mutual Information (MI), a concept core to 
InfoGAN. MI extends entropy to the amount of additional information you yield from 
observing a joint sample of two random variables as compared to the baseline of 
observing each variable separately.</p>

<p><strong>Topics</strong>:</p>
<ol>
  <li>Entropy</li>
  <li>Differential Entropy</li>
  <li>Conditional Entropy</li>
  <li>Jensen’s Inequality</li>
  <li>KL divergence</li>
  <li>Mutual Information</li>
</ol>

<p><strong>Required Reading</strong>:</p>
<ol>
  <li>Chapter 1.6 from <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning / Bishop. (“PRML”)</a></li>
  <li>A good <a href="https://www.quora.com/What-is-an-intuitive-explanation-of-the-concept-of-entropy-in-information-theory/answer/Peter-Gribble">intuitive explanation of Entropy</a>, from Quora.</li>
</ol>

<p><strong>Optional Reading</strong>:</p>
<ol>
  <li><a href="https://arxiv.org/pdf/1404.2000.pdf">Notes on Kullback-Leibler Divergence and Likelihood Theory</a></li>
  <li>For more perspectives and deeper dependencies, see Metacademy:
    <ol>
      <li><a href="https://metacademy.org/graphs/concepts/entropy">Entropy</a></li>
      <li><a href="https://metacademy.org/graphs/concepts/mutual_information">Mutual Information</a></li>
      <li><a href="https://metacademy.org/graphs/concepts/kl_divergence">KL divergence</a></li>
    </ol>
  </li>
  <li><a href="https://colah.github.io/posts/2015-09-Visual-Information/">Visual Information Theory</a></li>
</ol>

<p><strong>Questions</strong>:</p>
<ol>
  <li>From PRML: 1.31, 1.36, 1.37, 1.38, 1.39, 1.41.
    <details><summary>Solution</summary>
<p>
PRML 1.31: Consider two variables \(x\) and \(y\) having joint distribution \(p(x,y)\). Show that the differential entropy of this pair of variables satisfies \(H(x,y) \le H(x) + H(y)\) with equality if, and only if, \(x\) and \(y\) are statistically independent.
</p><p>
If \(p(x)\) and \(p(y)\) are independent then the joint distribution is given by:<br />
\(p(x,y) = p(x)p(y)\)
</p><p>
Based on the independent \(p(x)\) and \(p(y)\) the joint entropy can be derived from the conditional entropies \(H(x|y)\) and \(H(y|x)\):<br />
\(H(x|y) = H(x)\)<br />
\(H(y|x) = H(y) \to\)<br />
\(H(x,y) = H(x) + H(y|x) = H(y) + H(x|y) \to\)<br />
\(H(x,y) = H(x) + H(y)\)
</p><p>
Therefore, there is no mutual information \(I(x,y)\) if \(p(x)\) and \(p(y)\) are independent:<br />
\(H(x,y) = H(x) + H(y)\ \to\)<br />
\(I(x,y) = H(x) + H(y) - H(x,y) = H(x,y) - H(x,y) = 0\)
</p><p>
If \(p(x)\) and \(p(y)\) are dependent:<br />
\(H(x,y) &lt; H(x) + H(y)\ \to\)<br />
\(I(x,y) = H(x) + H(y) - H(x,y) &gt; 0\)
</p><p>
The indepent and dependent case can be combined to a general form:<br />
\(H(x,y) \le H(x) + H(y)\ \to\)<br />
\(I(x,y) = H(x) + H(y) - H(x,y) \ge 0\)
</p>
</details>
  </li>
  <li>How is Mutual Information similar to correlation? How are they different? Are they directly related under some conditions?
    <details><summary>Solution</summary>
<p>Start <a href="https://stats.stackexchange.com/questions/81659/mutual-information-versus-correlation">here</a>.
</p>
</details>
  </li>
  <li>In classification problems, <a href="https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k/4185">minimizing cross-entropy loss is the same as minimizing the KL divergence 
of the predicted class distribution from the true class distribution</a>. Why do we minimize the KL, rather
than other measures, such as L2 distance?
    <details><summary>Solution</summary>
<p>
In classification problem: One natural measure of “goodness” is the likelihood or marginal
probability of observed values. By definition, it’s \(P(Y | X; params)\), which is
\(\prod_i P(Y_i = y_i | X; params)\).
This says that we want to maximize the probability of producing the “correct” \(y_i\)
class only, and don’t really care to push down the probability of incorrect class like
L2 loss would.
</p><p>
E.g., suppose the true label \(y = [0, 1, 0]\) (one-hot of class label {1, 2, 3}),
and the softmax of the final layer in NN is \(y’ = [0.2, 0.5, 0.3]\).
One could use L2 between these two distributions, but if instead we minimize KL
divergence \(KL(y || y’)\), which is equivalent to minimizing cross-entropy
loss (the standard loss everyone uses to solve this problem),
we would compute \(0 \cdot \log(0) + 1 \cdot \log (0.5) + 0 \cdot \log(0) = \log(0.5)\),
which describes exactly the log likelihood of the label being class 2
for this particular training example.
</p><p>
Here choosing to minimize KL means we’re maximizing the data likelihood.
I think it could also be reasonable to use L2, but we would be maximizing
the data likelihood + “unobserved anti-likelihood” :) (my made up word)
meaning we want to kill off all those probabilities of predicting wrong
labels as well.
</p><p>
Another reason L2 is less prefered might be that L2 involves looping over all
class labels whereas KL can look only at the correct class when computing the loss.
</p>
</details>
  </li>
</ol>

<p><br /></p>

<h1 id="2-generative-adversarial-networks-gan">2 Generative Adversarial Networks (GAN)</h1>
<p><strong>Motivation</strong>: GANs are framework for constructing models that learn to sample 
  from a probability distribution, given a finite sample from that distribution.
  More concretely, after training on a finite unlabeled dataset (say of images), 
  a GAN can generate new images from the same “kind” that aren’t in the original
  training set.</p>

<p><strong>Topics</strong>:</p>
<ol>
  <li>JS (Jensen-Shannon) divergence</li>
  <li>How are GANs trained?</li>
  <li>Various possible GAN objectives. Why are they needed?</li>
  <li>GAN training minimizes the JS divergence between the data-generating distribution and the distribution of samples from the generator part of the GAN</li>
</ol>

<p><strong>Required Reading</strong>:</p>
<ol>
  <li><a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">JS Divergence</a></li>
  <li><a href="https://arxiv.org/abs/1406.2661">The original GAN paper</a></li>
</ol>

<p><strong>Optional Reading</strong>:</p>
<ol>
  <li><a href="https://arxiv.org/abs/1701.00160">NIPS 2016 Tutorial: Generative Adversarial Networks</a></li>
</ol>

<p><strong>Questions</strong>:</p>
<ol>
  <li>Prove that <script type="math/tex">0 \leq JSD(P||Q) \leq 1</script> bit for all P, Q. When are the bounds achieved?
    <details><summary>Solution</summary>
<p>Start <a href="https://en.wikipedia.org/wiki/Jensen-Shannon_divergence#Relation_to_mutual_information">here</a>.
</p>
</details>
  </li>
  <li>What are the bounds for KL divergence? When are those bounds achieved?
    <details><summary>Solution</summary>
<p>Start <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">here</a>.
</p><p>
The Kullback–Leibler divergence \(D_{KL}(P||Q)\) between \(P\) and \(Q\) is defined as:<br />
\(D_{KL}(P||Q) =  \sum_{x}P(x) \log_2\left(\frac{P(x)}{Q(x)}\right)\)
     </p><p>
The lower bound is reached when \(P(x) = Q(x)\) because \(\left(\frac{P(x)}{Q(x)}\right) = 1\):<br />
\(D_{KL}(P||Q) = \sum_{x}P(x) \log_2(1) = \sum_{x}P(x) 0 = 0\)
     </p><p>
The upper bound is reached when \(Q(x)\) is disjoint from \(P(x)\), i.e., \(Q(x)\) is zero  where \(P(x)\) is not zero, because then the log-ratio \(\log_2\left(\frac{P(x)}{Q(x)}\right)\) becomes \(\infty\):<br />
\(x_i \in x\)<br />
\(P(x_i) \log_2\left(\frac{P(x_i)}{Q(x_i)}\right) = P(x_i) \log_2\left(\frac{P(x_i)}{0}\right) = \infty \to\)<br />
\(D_{KL}(P||Q) =  \sum_{x}P(x) \log_2\left(\frac{P(x)}{Q(x)}\right) = \infty\)
</p>
</details>
  </li>
  <li>In the paper, why do they say “In practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, <script type="math/tex">log(1 − D(G(z)))</script> saturates”?
    <details><summary>Solution</summary>
  <p><a href="/assets/gan_gradient.pdf">Understanding the vanishing generator gradients point in the GAN paper</a></p>
</details>
  </li>
  <li>Implement a <a href="https://colab.research.google.com/">Colab</a> that trains a GAN for MNIST. Try both the saturating and non-saturating discriminator loss.
    <details><summary>Solution</summary>
<p>An implementation can be found <a href="https://colab.research.google.com/drive/1joM97ITFowvWU_qgRjQRiOKajHQKKH80#forceEdit=true&amp;offline=tru&amp;sandboxMode=true">here</a>.
</p>
</details>
  </li>
</ol>

<p><br /></p>

<h1 id="3-the-paper">3 The Paper</h1>
<p><strong>Motivation</strong>: Let’s read the <a href="https://arxiv.org/abs/1606.03657">paper</a>. Keep
  in mind that InfoGAN modifies the original GAN objective in this way:</p>
<ol>
  <li>Split the incoming noise vector z into two parts - noise and code. The goal 
is to learn meaningful codes for the dataset.</li>
  <li>In addition to the discriminator, it adds another prediction head to the 
network that tries to predict the code from the generated sample. The loss 
is a combination of the normal GAN loss and the prediction loss.</li>
  <li>This new loss term can be interpreted as a lower bound on the mutual 
information between the generated samples and the code.</li>
</ol>

<p><strong>Topics</strong>:</p>
<ol>
  <li>The InfoGAN objective</li>
  <li>Why can’t we directly optimize for the mutual information <script type="math/tex">I(c; G(z,c))</script></li>
  <li>Variational Information Maximization</li>
  <li>Possible choices for classes of random variables for dimensions of the code c</li>
</ol>

<p><strong>Reproduce</strong>:</p>

<p><a href="https://colab.research.google.com/drive/1JkCI_n2U2i6DFU8NKk3P6EkPo3ZTKAaq#forceEdit=true&amp;offline=true&amp;sandboxMode=true" class="colab-root">Reproduce in a
    <span>Notebook</span></a></p>

<p><strong>Required Reading</strong>:</p>
<ol>
  <li><a href="https://arxiv.org/abs/1606.03657">InfoGAN</a></li>
  <li><a href="http://aoliver.org/assets/correct-proof-of-infogan-lemma.pdf">A correction to a proof in the paper</a></li>
</ol>

<p><strong>Optional Reading</strong>:</p>
<ol>
  <li><a href="https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd">A blog post explaining InfoGAN</a></li>
</ol>

<p><strong>Questions</strong>:</p>
<ol>
  <li>How does one compute <script type="math/tex">log Q(c|x)</script> in practice? How does this answer change based on the choice of the type of random variables in c?
    <details><summary>Solution</summary>
  <p>What is \(\log Q(c|x)\) when c is a Gaussian centered at \(f_\theta(x)\)? What about when c is the output of a softmax?
  </p><p>
  See section 6 in the paper.
  </p> 
</details>
  </li>
  <li>Which objective in the paper can actually be optimized with gradient-based algorithms? How? (An answer to this needs to refer to “the reparameterization trick”)</li>
  <li>Why is an auxiliary <script type="math/tex">Q</script> distribution necessary?</li>
  <li>Draw a neural network diagram for InfoGAN
    <details><summary>Solution</summary>
  <p>There is a good diagram in <a href="https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd">this blog post</a></p>
</details>
  </li>
  <li>In the paper they say “However, in this paper we opt for
simplicity by fixing the latent code distribution and we will treat <script type="math/tex">H(c)</script> as a constant.”. What if you want to learn
the latent code (say, if you don’t know that classes are balanced in the dataset). Can you still optimize for this with gradient-based algorithms? Can you implement this on an intentionally class-imbalanced variant of MNIST?
    <details><summary>Solution</summary>
<p>
You could imagine learning the parameters of the distribution of c, if you can get H(c) to be a differentiable function of those parameters.
</p>
</details>
  </li>
  <li>In the paper they say “the lower bound … is quickly maximized to … and maximal mutual information is achieved”. How do they know this is the maximal value?</li>
  <li>Open-ended question: Is InfoGAN guaranteed to find disentangled representations? How would you tell if a representation is disentangled?</li>
</ol>
